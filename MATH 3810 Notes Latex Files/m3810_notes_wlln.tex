%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[11pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{color}
\usepackage{verbatim}
\usepackage{float}
\usepackage{multicol}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, calc}
\usetikzlibrary{decorations.pathmorphing}
\usepackage{tcolorbox}
\tcbuselibrary{breakable}

\newtcolorbox{solutionbox}{
  breakable,
  colback=blue!5!white,
  colframe=blue!50!black,
  title=Solution,
  sharp corners,
  boxrule=0.8pt
}

\newtcolorbox{hintbox}{
  breakable,
  colback=gray!10!white,
  colframe=gray!50!black,
  title=Hint,
  sharp corners,
  boxrule=0.5pt
}

% Unnumbered theorem
\newtheorem*{thm*}{Theorem}


\lstdefinelanguage{R}{
      keywords={if,else,while,for,in,next,break,function,TRUE,FALSE,NULL,Inf,NA,NaN,switch,repeat,return,require,library},
      keywordstyle=\color{blue}\bfseries,
      identifierstyle=\color{black},
      comment=[l]{\#},
      commentstyle=\color{gray}\ttfamily,
      string=[b]{"},
      stringstyle=\color{red}\ttfamily,
      morecomment=[l]{//},
      morestring=[b]{'},
      sensitive=true,
      morekeywords={print,summary,plot,lm,glm,data,frame,read.csv,write.csv,factor,levels,names,colnames,rownames,
      head,tail,str,dim,length,class,typeof,mode,is.na,is.null,is.finite,is.infinite,is.nan,as.numeric,as.character,
      as.factor,as.Date,as.POSIXct,as.matrix,as.data.frame,rbind,cbind,merge,subset,aggregate,tapply,apply,lapply,sapply,
      mapply,vapply,replicate,seq,rep,c,list,matrix,array,data.frame,table,hist,boxplot,barplot,pie,curve,lines,points,text,
      abline,legend,par,mtext,title,xlab,ylab,xlim,ylim,main,sub,col,pch,cex,lty,lwd,type,bg,fg,args,options,warnings,errors,
      message,stop,warning,error,try,tryCatch,withCallingHandlers,on.exit,debug,browser,trace,recover,options,getOption,setOption},
    }


\setlength{\textheight}{9in}
\setlength{\textwidth}{6in}
\addtolength{\topmargin}{-2cm}
\addtolength{\oddsidemargin}{-1cm}
\parindent=0in


\input{classinfo}
\input{latexmacros4810}

\vfuzz2pt % Don't report over-full v-boxes if over-edge is small
\hfuzz2pt % Don't report over-full h-boxes if over-edge is small

\renewcommand{\ni}{\noindent}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagestyle{myheadings}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%   Document Body   %%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\def\classnum{3810}
%\def\classtitle{Probability}
%\def\classtitleshort{Probability}
%\def\classsec{001}
%\def\classterm{Fall 2025}
%\def\instructor{Robert Rostermundt}
\def\printsol{0}
\def\printsol{0}


	\title{\vspace{-1in}Math\classnum\;-\;\classtitle\\
	Section\;\classsec\;-\;\classterm\\
	Notes: Weak Law of large Numbers}
	\author{University of Colorado Denver / College of Liberal Arts 	and Sciences}
	\date{Department of Mathematics - \instructor}

	\markright{Math\classnum\;-\;\classtitleshort, UCD, \classterm, \instructor}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}\maketitle\thispagestyle{empty}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace*{2mm}
\hrule
\vskip 8mm


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{The Problem:}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\noindent In probability and statistics, given some random variable $X$, we are frequently interested in knowing the expected value $E[X]$. If we know the distribution and have a known density function $f_{_X}$ (in the case of a continuous random variable) then we can compute this directly as
\[E[X]=\ds\int^{\infty}_{-\infty}x\cdot f_{_X}(x)\,dx.\]
But we will frequently be working with an unknown distribution. Or perhaps we believe that we are working with a normal distribution $N(\mu,\sigma^2)$, but we do not know the expected value $\mu$. In this case we will typically estimate the expected value with a sample mean
\[\overline{X}_n=\ds\frac{X_1+X_2+\cdots+X_n}{n}.\]
But we must ask several questions: 1) How large should the size of the sample be? 2) Will larger values of $n$ produce ``better" estimates?
\vskip 5mm
The Weak Law of Large Numbers provides partial answers to these questions. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Theoretical Tools:}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent The Weak Law of Large Numbers states that $\overline{X}_n$ {\bf\emph{converges in probability}} to $\mu=E[X]$. What is convergence in probability? It is a notion of a sequence of random variables $\{X_n\}$ becoming ``close to" another ransom variable $X$. Here is the formal definition.

\begin{defn}
We say that the sequence of random variables $\{X_n\}$ {\bf\emph{converges in probability}} to $X$ if, for all $\epsilon>0$, we have
\[\ds\lim_{n\to\infty}\P\left(\big|X_n-X\big|>\epsilon\right)=0.\]
We denote this as $X_n\stackrel{p}{\longrightarrow}X$.
\end{defn}
\vskip 5mm
For the Weak Law of Large Numbers the sequence $\{\overline{X}_n\}$ is converging to a constant random variable $\mu$. The statement of the theorem follows.

\vfill\eject

\begin{thm}[WLLN]
Let $X$ be a random variable with finite expected value; i.e., $\mu=E[X]<\infty$, and $X_1,X_2,\dots,X_n$ be an independent random sample from the population with distribution $X$. If we define the sample mean $\overline{X}_n=\ds\frac{X_1+\cdots+X_n}{n}$, we have $\overline{X}_n\stackrel{p}{\longrightarrow}\mu$; i.e., the sample mean converges in probability to the expected value $\mu$.
\end{thm}

\ni The following graphic should demonstrate the idea. Here we are sampling from a standard normal distribution $N(0,1)$ and looking at the distribution of the sample means.

\vskip 5mm

\begin{figure}[h!]
\begin{center}
\includegraphics[trim=0cm 0cm 0cm 0cm, clip=true, scale=0.5]{wlln_graphic.jpeg}
\caption{Weak Law of Large Numbers}
\end{center}
\end{figure}

\vskip 5mm
We can see the probability of the sampling distribution concentrating about the true mean $E[X]=0$ as the sample size $n$ increases. Below are two more simulations of convergence in probability for the Weak Law of Large Numbers.

\begin{figure}[h!]
\begin{center}
\includegraphics[trim=0cm 2cm 0cm 2cm, clip=true, scale=0.18]{wlln_plot_1.jpeg}\\
\caption{WLLN - Sample Sizes of $n\le 2000$}
\end{center}
\end{figure}

\vfill\eject

\begin{figure}[h!]
\begin{center}
\includegraphics[trim=0cm 2cm 0cm 2cm, clip=true, scale=0.18]{wlln_plot_2.jpeg}\\
\caption{WLLN - Sample Sizes of $n\le 2000$}
\end{center}
\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Proofs:}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The standard proof of the WLLN rests on two results.

\begin{thm}[Markov Inequality] Suppose $X\ge 0$ is a non-negative random variable and $a>0$. If $E[X]$ is finite then
\[E[X]\ge a\cdot\P\big(X\ge a\big).\]
\end{thm}
\vskip 5mm
What does this tell us? For one thing, it states that if $E[X]$ is ``small," then the probability that $X$ is ``large" (in the right tail) is ``small." A direct consequence of this is the following well-known theorem.
\vskip 5mm
\begin{thm}[Chebyshev]
Let $X$ be a random variable with finite expected value $E[X]=\mu$ and variance $Var(X)=\sigma^2$. Then for any positive number $a$ we have
\[\P\big(|X-\mu|\ge a\big)\le\ds\frac{\sigma^2}{a^2}.\]
\end{thm}
	\begin{proof}
Consider the non-negative random variable $(X-\mu)^2\ge 0$. Then we have that $E\left[\big(X-\mu\big)^2\right]=Var[X]=\sigma^2$. Then by the Markov Inequality, if $a$ is a positive real number we have
\beq
Var[X]&=&E\left[\big(X-\mu\big)^2\right]\\
&\ge&a^2\cdot\P\left(\big(X-\mu\big)^2\ge a^2\right)\\
&=&a^2\cdot\P\left(\big|X-\mu\big|\ge a\right)\\
\eeq
From this we conclude that
\[\P\big(|X-\mu|\ge a\big)\le\ds\frac{\sigma^2}{a^2}.\]
	\end{proof}

\noindent What does this tell us? It says that if $Var[X]=\sigma^2$ is ``small," then most of the probability will be concentrated around the mean $\mu=E[X]$.
\vskip 5mm
So how does this guarantee the Weak Law of Large Numbers? If we consider a random sample $\overline{X}_n$ from a distribution from a population described by a random variable $X$ with $E[X]=\mu$ and $Var[X]=\sigma^2$, then we know that $E\left[\overline{X}_n\right]=\mu$ and $Var\left[\overline{X}_n\right]=\sigma^2/n$. Then by Chebyshev we have for any positive real number $\epsilon$ that
\[\P\left(\big|\overline{X}_n-\mu\big|\ge\epsilon\right)\le\ds\frac{\sigma^2}{n\epsilon^2}.\]
From here it is a straightforward conclusion that $\ds\lim_{n\to\infty}\P\left(\big|\overline{X}_n-\mu\big|>\epsilon\right)=0$.
\vskip 5mm
While this is nice to have very small probabilities of $\epsilon$-excursions for large sample means, fortunately, we have an even stronger result in the Strong Law of Large Numbers.
\vskip 5mm
\begin{thm}[SLLN]
Let $X$ be a random variable with finite expected value; i.e., $\mu_{_X}=E[X]<\infty$, and $X_1,X_2,\dots,X_n$ be an independent random sample from the population with distribution $X$. If we define the sample mean $\overline{X}_n=\ds\frac{X_1+\cdots+X_n}{n}$, we have $\overline{X}_n\stackrel{a.s.}{\longrightarrow}\mu_{_X}$; i.e., the sample mean converges to the expected value $\mu_{_X}$ almost surely.\\
\end{thm}
Notice that this is a different type of convergence.
\begin{defn}
Let $(\O,\F,\P)$ be a probability space and $\{X_n\}$ be a sequence of random variables and $X$ another random variable relative to this probability space. We say the sequence $\{X_n\}$ {\bf\emph{converges almost surely to}} $X$ (or converges with probability one), and denote this as $X_n\stackrel{a.s}{\longrightarrow}X$, if
\[\P\left(\Big\{\o\in\O:\ds\lim_{n\to\infty}X_n(\o)=X(\o)\Big\}\right)=1.\]  
\end{defn}
\ni The difference between the WLLN and the SLLN are subtle. Moreover, the proof of the SLLN is beyond the scope of these notes. But the SLLN guarantees that for any $\epsilon$ tolerance, there will only be a finite number of values of the sample mean $\overline{X}_n$ that will fall outside of the tolerance-band. Fortunately the WLLN is typically strong enough for most applications.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{The R Code:} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 Here is the R code used for the above simulations.
\vspace*{2mm}
\small 
\begin{lstlisting}[language=R]

# ==========================================
# Weak Law of Large Numbers Demonstration
# ==========================================

set.seed(123)   # for reproducibility

k <- 1000       # number of observations per sample
j <- 50         # number of independent sequences
e <- 0.04       # epsilon threshold

# Generate matrix of random variables
x <- matrix(2 * rbinom(k * j, size = 1, prob = 0.5) - 1, ncol = j)

# Compute running averages for each column
y <- apply(x, 2, function(z) cumsum(z) / seq_along(z))

# Plot running averages
matplot(y, type = "l",
        lty = 1, col = rainbow(j),
        ylim = c(-0.4, 0.4),
        main = "Weak Law of Large Numbers Simulation",
        xlab = "Number of trials (n)",
        ylab = "Running average of Xn")

# Add horizontal lines for epsilon and 0
abline(h = c(-e, e), lty = 2, lwd = 2, col = "red")
abline(h = 0, lty = 1, lwd = 2, col = "black")

# Add legend
legend("topright",
       legend = c(expression(paste("|X| <", epsilon)), "Mean (0)"),
       col = c("red", "black"),
       lty = c(2, 1),
       bty = "n")



#=========================
# WLLN Wisualization 
#=========================
set.seed(2025)

M <- 5000
true_mean <- 0
true_sd <- 1
n_values <- c(1, 2, 5, 10, 30, 100, 500, 2000)

xlim <- c(-3, 3)
ylim <- c(0, 2.5)

# Adjust margins: give more space below (oma[1]) and above (oma[3])
par(mfrow = c(2, 4), mar = c(2.5, 3, 2, 1), oma = c(4, 0, 3, 0))

for (n in n_values) {
  sample_means <- replicate(M, mean(rnorm(n, mean = true_mean, sd = true_sd)))
  
  hist(sample_means,
       main = paste("n =", n),
       xlab = "",  # remove individual x labels; we'll add a global one
       col = "lightblue", border = "white",
       breaks = 30, freq = FALSE,
       xlim = xlim, ylim = ylim)
  
  curve(dnorm(x, mean = true_mean, sd = true_sd / sqrt(n)),
        col = "red", lwd = 2, add = TRUE)
  
  abline(v = true_mean, col = "darkblue", lwd = 2, lty = 2)
}

# global labels and title
mtext("Sample mean", side = 1, outer = TRUE, line = 2.5, cex = 1)
mtext("Distributions of Sample Means - Convergence to True Mean (WLLN)", 
      outer = TRUE, line = 1, cex = 1.2)



\end{lstlisting}



\vskip 1cm
\hrule
\vskip 5mm
\begin{center}
\bf Please let me know if you have any questions, comments, or corrections!
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
